{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of REAL and FAKE values\n",
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='label', order=df['label'].value_counts().index, palette = \"magma_r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real vs fake\n",
    "fig = px.pie(df,names='label',title='Proportion of Real vs. Fake News', color_discrete_sequence=px.colors.sequential.RdBu)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "labels = df.label\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.title('Relationship between length of the text article and its label',fontsize=20)\n",
    "\n",
    "# Extract the text and label columns\n",
    "texts = df['text']\n",
    "labels = df['label']\n",
    "\n",
    "# Extract the length of the text column\n",
    "lengths = texts.apply(lambda x: len(x))\n",
    "\n",
    "# Map each label to a color\n",
    "colors = labels.apply(lambda x: 'green' if x == 'REAL' else 'red')\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(lengths, labels, c=colors)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Rejoin the stemmed tokens into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the text data\n",
    "x_train_preprocessed = x_train.apply(preprocess)\n",
    "x_test_preprocessed = x_test.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing preprocessed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word cloud for real and fake preprocessed data\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Extract the preprocessed text and label columns from the dataset\n",
    "texts = df['preprocessed_text']\n",
    "labels = df['label']\n",
    "\n",
    "# Create a string containing the preprocessed text data for \"real\" news articles\n",
    "real_text = ' '.join([text for text, label in zip(texts, labels) if label == 'REAL'])\n",
    "\n",
    "# Create a word cloud object for \"real\" news articles\n",
    "real_wordcloud = WordCloud().generate(real_text)\n",
    "\n",
    "# Create a string containing the preprocessed text data for \"fake\" news articles\n",
    "fake_text = ' '.join([text for text, label in zip(texts, labels) if label == 'FAKE'])\n",
    "\n",
    "# Create a word cloud object for \"fake\" news articles\n",
    "fake_wordcloud = WordCloud().generate(fake_text)\n",
    "\n",
    "# Display the word cloud for \"real\" news articles\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(real_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Real News',fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Display the word cloud for \"fake\" news articles\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(fake_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Fake News',fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform train set, transform test set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(x_train_preprocessed) \n",
    "tfidf_test=tfidf_vectorizer.transform(x_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_test.shape)\n",
    "print(tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training, Evaluation, and Prediction along with Classification report and Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PassiveAggressiveClassifier\n",
    "model1 = PassiveAggressiveClassifier(max_iter=50)\n",
    "model1.fit(tfidf_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate accuracy\n",
    "y_pred1 = model1.predict(tfidf_test)\n",
    "score1 = accuracy_score(y_test,y_pred1)\n",
    "print(f'Accuracy: {round(score1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred1)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix for PassiveAggressiveClassifier\n",
    "cm1 = confusion_matrix(y_test,y_pred1, labels=['FAKE','REAL']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Confusion Matrix for PassiveAggressiveClassifier'+ \"\\033[0m\")\n",
    "plot_confusion_matrix(conf_mat=cm1,show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,class_names=['FAKE','REAL'], cmap=plt.cm.magma_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LogisticRegression\n",
    "model2 = LogisticRegression(max_iter=50)\n",
    "model2.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate accuracy\n",
    "y_pred2 = model2.predict(tfidf_test)\n",
    "score2 = accuracy_score(y_test,y_pred2)\n",
    "print(f'Accuracy: {round(score2, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred2)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix for LogisticRegression\n",
    "cm2 = confusion_matrix(y_test,y_pred2, labels=['FAKE','REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Confusion Matrix for LogisticRegression'+ \"\\033[0m\")\n",
    "plot_confusion_matrix(conf_mat=cm2,show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,class_names=['FAKE','REAL'], cmap=plt.cm.magma_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DecisionTreeClassifier\n",
    "model3 = DecisionTreeClassifier()\n",
    "model3.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate accuracy\n",
    "y_pred3 = model3.predict(tfidf_test)\n",
    "score3 = accuracy_score(y_test,y_pred3)\n",
    "print(f'Accuracy: {round(score3, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred3)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix for DecisionTreeClassifier\n",
    "cm3 = confusion_matrix(y_test,y_pred3, labels=['FAKE','REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Confusion Matrix for DecisionTreeClassifier'+ \"\\033[0m\")\n",
    "plot_confusion_matrix(conf_mat=cm3,show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,class_names=['FAKE','REAL'], cmap=plt.cm.magma_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomforestClassifier\n",
    "model4 = RandomForestClassifier()\n",
    "model4.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate accuracy\n",
    "y_pred4 = model4.predict(tfidf_test)\n",
    "score4 = accuracy_score(y_test,y_pred4)\n",
    "print(f'Accuracy: {round(score4, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred4)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix for RandomForestClassifier\n",
    "cm4 = confusion_matrix(y_test,y_pred4, labels=['FAKE','REAL']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Confusion Matrix for RandomForestClassifier'+ \"\\033[0m\")\n",
    "plot_confusion_matrix(conf_mat=cm4,show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,class_names=['FAKE','REAL'], cmap=plt.cm.magma_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Initializa SupportVectorMachineClassifier\n",
    "model5 = SVC()\n",
    "model5.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate accuracy\n",
    "y_pred5 = model5.predict(tfidf_test)\n",
    "score5 = accuracy_score(y_test,y_pred5)\n",
    "print(f'Accuracy: {round(score5, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred5)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix for RandomForestClassifier\n",
    "cm5 = confusion_matrix(y_test,y_pred5, labels=['FAKE','REAL']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Confusion Matrix for SupportVectorMachine'+ \"\\033[0m\")\n",
    "plot_confusion_matrix(conf_mat=cm5,show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,class_names=['FAKE','REAL'], cmap=plt.cm.magma_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={'PA Classifier':score1,'LR':score2,'DT Classifier':score3,'RF Classifier':score4,'SVM':score5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Accuracy Comparison of ML Models Bar Chart',fontsize=20)\n",
    "colors=['lightblue','lightblue','lightblue','lightblue','lightblue']\n",
    "plt.xticks(fontsize=10,color='midnightblue')\n",
    "plt.yticks(fontsize=16,color='midnightblue')\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.xlabel('Models',fontsize=16)\n",
    "plt.bar(labels.keys(),labels.values(),edgecolor='black',color=colors, linewidth=2,alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Accuracy Comparison of ML Models Line Plot',fontsize=20)\n",
    "\n",
    "# Extract the model accuracies\n",
    "accuracies = [score1, score2, score3, score4,score5]\n",
    "\n",
    "# Extract the names of the model\n",
    "model_names = ['Passive Aggressive Classifier', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier','Support Vector Machine']\n",
    "\n",
    "# Plot the accuracy values\n",
    "plt.plot(model_names, accuracies)\n",
    "\n",
    "# Add a legend and label the axes\n",
    "plt.legend(['Accuracy'])\n",
    "plt.xlabel('Model', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
